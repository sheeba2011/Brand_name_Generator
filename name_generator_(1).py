# -*- coding: utf-8 -*-
"""Name_Generator (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ApKaxCK2kLN9hkwFOPF0jQftSG4UIIpN

# Name Generation using RNN
Generate new names using GRU network. Here a character level model has been used for this task.
"""

import tensorflow as tf
from numpy.random import shuffle
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split

"""##  Data loading"""

# for loading data
def load_data(filename):
    with open(filename, 'r') as f:
        data = f.read().lower().split('\n')
    return np.array(data)

from google.colab import drive
drive.mount('/content/drive')

data = load_data('/content/drive/MyDrive/names.txt')
print('No. of names: ' + str(len(data)))

"""Split data into training and validation set"""

# split data
train_data, val_data = train_test_split(data, test_size=0.05)
print(len(train_data))
print(len(val_data))

"""## Text Processing
We will use a character level model for this task. So first we will create a vocabulary dictionary. Since this is a character level model so the only the vocabulary size is fixed.
"""

# find the unique characters for the vocabulary
#  Here '#' means End of Sentence
vocab_tokens = ['#'] + list(set(''.join(data)))
vocab_tokens_len = len(vocab_tokens)
print(vocab_tokens_len)

# find the max length amongst names
max_name_len = max(len(name) for name in train_data)
print('Max length: ', max_name_len)

"""For training the model we need to represent the string characters in terms of numbers. For that we will create a dictionary mapping of characters and numerical indices and vice-versa."""

# create vocabulary dict
# character to index
ch_to_idx = {ch:idx for idx, ch in tqdm(enumerate(vocab_tokens))}
print('\nDict len: ', len(ch_to_idx))
print(ch_to_idx)

# index to chracter
idx_to_ch = {ch_to_idx[ch]:ch for ch in ch_to_idx.keys()}
print('\nDict len: ', len(idx_to_ch))
print(idx_to_ch)

# for converting string names to matrix representation consisting of numerical indices
# output shape: (m, Tx)
def to_num_matrix(names, ch_to_idx):

    max_len = max(map(len, names))
    names_idx = np.zeros((len(names), max_len), dtype='int32')

    for i, name in enumerate(names):
        for j, ch in enumerate(name):
            names_idx[i][j] = ch_to_idx[ch]

    return names_idx

to_num_matrix(train_data, ch_to_idx).shape

"""## Training
This is a simple model using Vanilla Recurrent units.
"""

from IPython.display import clear_output
from random import sample
import pickle
import sys

from keras.layers import Embedding, Dense, Concatenate
import keras
import keras.layers as L
from keras.utils import to_categorical
from keras.callbacks import LambdaCallback, ModelCheckpoint
from keras.models import Sequential, load_model

# for finding the validation accuracy
def compute_acc():
    # convert validation data
    val_x = to_num_matrix(val_data, ch_to_idx)
    # validation labels
    val_y = np.copy(val_x[:,1:])


    # leave the last character
    pred = model.predict(val_x[:,:-1], batch_size=256, verbose=1)
    pred_ch = pred.argmax(axis=-1)

    # find acc
    return float(np.sum(np.logical_and((pred_ch!=0), (pred_ch == val_y)))) \
                / np.sum(val_y != 0)

# custom callback
def acc_on_epoch_end(epoch, logs):
    sys.stdout.flush()
    print('\nValidation Accuracy: ' + str(compute_acc()*100) + ' %')
    sys.stdout.flush()

# for generating batches
def generate_model_batches(names, batch_size=32, pad=0):
    # no. of training examples
    m = np.arange(len(names))

    while True:
        # get a shuffled index list
        idx = np.random.permutation(m)

        # start yeilding batches
        for start in range(0, len(idx)-1, batch_size):
            batch_idx = idx[start:start+batch_size]
            batch_words = []

            # take out the words and tags from 'batch_size' no. of training examples
            for index in batch_idx:
                batch_words.append(names[index])

            # input x
            batch_x = to_num_matrix(batch_words, ch_to_idx)

            # output labels
            batch_y_ohe = to_categorical(batch_x[:,1:], len(vocab_tokens))
            yield batch_x[:,:-1], batch_y_ohe

"""## Model
We will use a simple GRU model.
"""

acc_callback = LambdaCallback(on_epoch_end=acc_on_epoch_end)
checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/best_weights.h5', verbose=1, save_best_only=True)

model = Sequential()
model.add(L.InputLayer([None], dtype='int32'))
# embeddings layer
model.add(L.Embedding(len(vocab_tokens), 20))

# gru layer
model.add(L.GRU(128, return_sequences=True, activation='tanh'))
model.add(L.Dropout(0.5))
model.add(L.BatchNormalization())

# apply softmax
model.add(L.TimeDistributed(L.Dense(len(vocab_tokens), activation='softmax')))

model.summary()

BATCH_SIZE = 256

# select optimizer
optimizer = keras.optimizers.Adam(clipvalue=1.2)
model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# train model
hist = model.fit_generator(generate_model_batches(train_data, batch_size=BATCH_SIZE), steps_per_epoch=len(train_data)/BATCH_SIZE,
                          callbacks=[acc_callback, checkpoint], epochs=1000)

# save model
model.save('/content/drive/MyDrive/Colab Notebooks/model.h5')
model.save_weights('/content/drive/MyDrive/Colab Notebooks/weights.h5')

hist.history

# loss plot
plt.figure(figsize=(16,5))
plt.subplot(1,1,1)
plt.suptitle('Optimizer: Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
plt.plot(hist.history['loss'], color='b', label='Training Loss')
plt.legend(loc='upper right')
plt.savefig('/content/drive/MyDrive/Colab Notebooks/loss.png')

# acc plot
plt.figure(figsize=(16,5))
plt.subplot(1,1,1)
plt.suptitle('Optimizer: Adam', fontsize=10)
plt.ylabel('Accuracy', fontsize=16)
plt.plot(hist.history['accuracy'], color='b', label='Training Loss')
plt.legend(loc='lower right')

plt.savefig('/content/drive/MyDrive/Colab Notebooks/acc.png')
plt.show()

"""## Sample Text
Generate new names by feeding a seed input.
"""

# samples text from language model
def sample_text(seed_phrase='n', max_length=max_name_len):


    # input x to numerical index
    text_x = to_num_matrix(list(list(seed_phrase)), ch_to_idx)
    # index of current character
    ch = ch_to_idx[seed_phrase]

    # list of predicted character indices
    indices = []
    # counter
    i = 0

    #start generating
    while ch != 0 and i<max_length:
        # predict using the input character
        pred = model.predict(text_x)[0]

        # get the char with highest prob.
        ch = np.random.choice(np.arange(len(vocab_tokens)) , p = pred.ravel())

        if ch!=0:
            indices.append(ch)

        # feed the current char as the next input
        text_x = to_num_matrix(list(list(idx_to_ch[ch])), ch_to_idx)

        i = i+ 1


    return ''.join([idx_to_ch[idx] for idx in indices])

i = 0
while i < 18:
    name = sample_text(max_length=6)
    if name != '':
        print(name)
        i += 1

